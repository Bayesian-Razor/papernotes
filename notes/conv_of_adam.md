# On the Convergence of Adam and Beyond

## 背景

有很多人都吐槽过`Adam`的问题。这篇文章就构造了`Adam`收敛错过最优解的情况，以及提出了改进的方法。

## 大体内容

`Adam`方法大概可以总结成分别在梯度和梯度的按元素平方的两个方向上做EWMA，然后应用梯度。由于`Adam`这类方法，同时使用`梯度按元素平方的EWMA`我们简称为二阶动量来调整步长，这个时候如果导数的震动幅度很大，这个时候如果有可能导致学习率的上升。这就使得迭代后期（我们期望学习率或者说步长变小的时候）面临模型不收敛的情况。由于`beta2`用来调整二阶梯度计算时历史的影响力，所以一个大的`beta2`可能有帮助。

不过文中给出了他们自己的优化方法，既然本质是由于步长的不收敛导致的。就让每次求v的时候，让他取上次的v和这次的v之间大。来保证步长的收敛。

## 其他想法

最后的实验，真实数据集很少，而且只用和`Adam`比，具体调参时候使用的难度还不好评估。